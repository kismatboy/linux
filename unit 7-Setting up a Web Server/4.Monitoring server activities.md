Monitoring server activities is crucial for ensuring the health, performance, and security of a server. There are various tools and techniques available for monitoring server activities on Linux systems like Ubuntu. Here's a general guide on how to monitor server activities:

**1. System Monitoring Tools:**

There are several built-in command-line tools available in Ubuntu for monitoring various aspects of your server:

- **`top`**: Provides real-time information about system processes, CPU usage, memory usage, and more. It's particularly useful for tracking resource-intensive processes.

- **`htop`**: An enhanced version of `top` that offers a more user-friendly interface and additional features like scrolling and process sorting.

- **`free`**: Displays information about system memory (RAM) usage, including total, used, and available memory.

- **`vmstat`**: Provides statistics about system processes, memory, and I/O activity. It's useful for diagnosing performance issues.

- **`iostat`**: Monitors input/output (I/O) statistics for storage devices, helping you identify disk performance bottlenecks.

- **`netstat`**: Shows network-related statistics, including active network connections, listening ports, and routing tables.

**2. Log Files:**

Log files contain valuable information about system activities, errors, and events. Key log files to monitor include:

- **`/var/log/syslog`**: Contains system messages, including kernel and hardware-related information.

- **`/var/log/auth.log`**: Logs authentication and authorization events, which can help you detect unauthorized access attempts.

- **`/var/log/nginx/access.log`**: If you're running a web server like Nginx, this log file records HTTP access requests.

- **`/var/log/apache2/access.log`**: For Apache web server, this log file contains access records.

- **`/var/log/mysql/error.log`**: Logs MySQL database errors and issues.

You can use text editors like `nano` or command-line tools like `cat`, `grep`, and `tail` to view and search log files.

**3. Monitoring Solutions:**

For more comprehensive server monitoring, consider using monitoring solutions like:

- **Prometheus**: An open-source monitoring and alerting toolkit that can collect and display metrics from various sources. Grafana can be used in conjunction with Prometheus to visualize the data.

- **Zabbix**: An enterprise-class open-source monitoring solution that can monitor various aspects of your server, including performance, availability, and network activity.

- **Nagios**: A widely used open-source monitoring system that can monitor network services, hosts, and server components.

- **ELK Stack (Elasticsearch, Logstash, Kibana)**: Useful for centralized logging and log analysis, allowing you to collect, analyze, and visualize log data.

**4. Resource Monitoring:**

Tools like `sar` (System Activity Reporter) can help you collect and report system activity information at regular intervals. To install `sar`, use:

```bash
sudo apt-get install sysstat
```

You can then run `sar` commands to monitor CPU, memory, disk, and network activity over time.

**5. Security Monitoring:**

Consider using intrusion detection systems (IDS) like `fail2ban` or security information and event management (SIEM) tools to monitor and respond to security events on your server.

**6. Alerts and Notifications:**

Configure alerting and notification systems to alert you when certain thresholds or events are reached. Tools like `cron` and `mail` can be used to schedule and send notifications.

**7. Regular Maintenance:**

Perform regular server maintenance tasks, such as software updates, security patches, and log rotation, to keep your server healthy and secure.

**analyze log using advance commands**

To analyze log files and extract meaningful data, you can use advanced text manipulation techniques. The specific commands will depend on the format of your log files and the information you want to extract. Below, We'll look at a general example using `awk` to extract useful information from an Apache access log file. Please adjust the commands according to your log file format and data extraction requirements.

Assuming you have an Apache access log file (`access.log`) with lines like this:

```
192.168.1.100 - - [26/Sep/2023:15:30:45 +0000] "GET /page1 HTTP/1.1" 200 1234
192.168.1.101 - - [26/Sep/2023:15:31:10 +0000] "GET /page2 HTTP/1.1" 404 0
```

And you want to extract the IP address, requested URL, response code, and response size into a CSV file, you can use the following `awk` command:

```bash
awk '{print $1, $7, $9, $10}' access.log > extracted_data.csv
```

Explanation of the `awk` command:

- `'{print $1, $7, $9, $10}'`: This command specifies the fields to be extracted from each line. `$1` is the first field (IP address), `$7` is the seventh field (requested URL), `$9` is the ninth field (response code), and `$10` is the tenth field (response size).

- `access.log`: This is the input log file.

- `> extracted_data.csv`: This part redirects the output to a CSV file named `extracted_data.csv`.

After running this command, `extracted_data.csv` will contain the extracted data in CSV format:

```
192.168.1.100 /page1 200 1234
192.168.1.101 /page2 404 0
```
**example 2**
in the above example we now we also want to get the time also like sept/26-15:31  in the csv first column
```bash
awk '{split($4, date, "[/:]"); time=date[2]"/"date[1]"-"date[3]; print time, $1, $7, $9, $10}' access.log > extracted_data.csv
```

Here's an explanation of the modified `awk` command:

- `split($4, date, "[/:]")`: This command splits the fourth field (the date and time) into an array called `date` using the characters "/" and ":" as separators.

- `time=date[2]"/"date[1]"-"date[3]`: This line constructs the desired time format (e.g., "Sep/26-15:31") by rearranging the elements from the `date` array.

- `print time, $1, $7, $9, $10`: This command prints the extracted time, IP address, requested URL, response code, and response size.

- `access.log`: This is the input log file.

- `> extracted_data.csv`: This part redirects the output to a CSV file named `extracted_data.csv`.

After running this modified command, the `extracted_data.csv` file will contain the extracted data with the time in the specified format as the first column:

```
Sep/26-15:30 192.168.1.100 /page1 200 1234
Sep/26-15:31 192.168.1.101 /page2 404 0
```

You can adjust the field positions and formatting according to your log file's structure and the specific information you need to extract. Keep in mind that log file formats can vary widely, so your `awk` or `sed` commands will need to be customized to match your specific log file structure and data extraction requirements.